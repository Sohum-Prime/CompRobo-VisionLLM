{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CompRobo LLM-based Visual Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, DetrImageProcessor, DetrForObjectDetection\n",
    "from PIL import Image\n",
    "import torch\n",
    "#\n",
    "import os\n",
    "from tempfile import NamedTemporaryFile\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionTool(BaseTool):\n",
    "    name = \"Image captioner\"\n",
    "    description = \"Use this tool when given the path to an image that you would like to be described. \" \\\n",
    "                  \"It will return a simple caption describing the image.\"\n",
    "\n",
    "    def _run(self, img_path):\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        model_name = \"Salesforce/blip-image-captioning-large\"\n",
    "        device = \"cpu\"  # cuda\n",
    "\n",
    "        processor = BlipProcessor.from_pretrained(model_name)\n",
    "        model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "        inputs = processor(image, return_tensors='pt').to(device)\n",
    "        output = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "        caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        return caption\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "\n",
    "class ObjectDetectionTool(BaseTool):\n",
    "    name = \"Object detector\"\n",
    "    description = \"Use this tool when given the path to an image that you would like to detect objects. \" \\\n",
    "                  \"It will return a list of all detected objects. Each element in the list in the format: \" \\\n",
    "                  \"[x1, y1, x2, y2] class_name confidence_score.\"\n",
    "\n",
    "    def _run(self, img_path):\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # convert outputs (bounding boxes and class logits) to COCO API\n",
    "        # let's only keep detections with score > 0.9\n",
    "        target_sizes = torch.tensor([image.size[::-1]])\n",
    "        results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "        detections = \"\"\n",
    "        for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "            detections += '[{}, {}, {}, {}]'.format(int(box[0]), int(box[1]), int(box[2]), int(box[3]))\n",
    "            detections += ' {}'.format(model.config.id2label[int(label)])\n",
    "            detections += ' {}\\n'.format(float(score))\n",
    "\n",
    "        return detections\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_caption(image_path):\n",
    "    \"\"\"\n",
    "    Generates a short caption for the provided image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: A string representing the caption for the image.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    model_name = \"Salesforce/blip-image-captioning-large\"\n",
    "    device = \"cpu\"  # cuda\n",
    "\n",
    "    processor = BlipProcessor.from_pretrained(model_name)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "    inputs = processor(image, return_tensors='pt').to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return caption\n",
    "\n",
    "\n",
    "def detect_objects(image_path):\n",
    "    \"\"\"\n",
    "    Detects objects in the provided image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with all the detected objects. Each object as '[x1, x2, y1, y2, class_name, confindence_score]'.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "    model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # convert outputs (bounding boxes and class logits) to COCO API\n",
    "    # let's only keep detections with score > 0.9\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "    detections = \"\"\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        detections += '[{}, {}, {}, {}]'.format(int(box[0]), int(box[1]), int(box[2]), int(box[3]))\n",
    "        detections += ' {}'.format(model.config.id2label[int(label)])\n",
    "        detections += ' {}\\n'.format(float(score))\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from getpass import getpass\n",
    "#set the openai_api_key\n",
    "openai_api_key = getpass()\n",
    "\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# import os\n",
    "# import openai\n",
    " \n",
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the agent\n",
    "tools = [ImageCaptionTool(), ObjectDetectionTool()]\n",
    "\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key= openai_api_key,\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    max_iterations=5,\n",
    "    verbose=True,\n",
    "    memory=conversational_memory,\n",
    "    early_stopping_method='generate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-27 14:42:51--  https://www.smartcitiesworld.net/AcuCustom/Sitename/DAM/019/Parsons_PR.jpg\n",
      "Resolving www.smartcitiesworld.net (www.smartcitiesworld.net)... 108.129.13.134, 54.76.134.71\n",
      "Connecting to www.smartcitiesworld.net (www.smartcitiesworld.net)|108.129.13.134|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 299237 (292K) [image/jpeg]\n",
      "Saving to: ‘Parsons_PR.jpg’\n",
      "\n",
      "Parsons_PR.jpg      100%[===================>] 292.22K  1.07MB/s    in 0.3s    \n",
      "\n",
      "2023-10-27 14:42:52 (1.07 MB/s) - ‘Parsons_PR.jpg’ saved [299237/299237]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#download the image\n",
    "!wget https://www.smartcitiesworld.net/AcuCustom/Sitename/DAM/019/Parsons_PR.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Image captioner\",\n",
      "    \"action_input\": \"/home/sohum/CompRobo-VisionLLM/Parsons_PR.jpg\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mcars are driving down the street in traffic at a green light\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The caption for the image is: 'cars are driving down the street in traffic at a green light'\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The caption for the image is: 'cars are driving down the street in traffic at a green light'\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/home/sohum/CompRobo-VisionLLM/Parsons_PR.jpg\"\n",
    "user_question = \"generate a caption for this iamge?\"\n",
    "response = agent.run(f'{user_question}, this is the image path: {image_path}')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Object detector\",\n",
      "    \"action_input\": \"/home/sohum/CompRobo-VisionLLM/Parsons_PR.jpg\"\n",
      "}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m[518, 40, 582, 110] car 0.9300956726074219\n",
      "[188, 381, 311, 469] car 0.9253584146499634\n",
      "[1068, 223, 1104, 342] person 0.9871610403060913\n",
      "[828, 233, 949, 329] car 0.9450380802154541\n",
      "[1076, 263, 1106, 347] bicycle 0.9070360660552979\n",
      "[635, 71, 713, 135] car 0.9211900234222412\n",
      "[0, 433, 100, 603] car 0.9781932830810547\n",
      "[151, 747, 339, 799] car 0.9839043617248535\n",
      "[389, 267, 493, 367] car 0.9801322817802429\n",
      "[192, 478, 341, 633] car 0.995318591594696\n",
      "[578, 117, 828, 550] traffic light 0.9860709309577942\n",
      "[802, 666, 1028, 798] car 0.982887327671051\n",
      "[0, 639, 84, 799] car 0.9630054831504822\n",
      "[1057, 608, 1199, 766] car 0.965281069278717\n",
      "[988, 218, 1031, 347] person 0.9471637606620789\n",
      "[751, 524, 909, 675] car 0.9911808371543884\n",
      "[489, 560, 670, 749] car 0.9970001578330994\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The items present in the image are: cars, persons, bicycles, and a traffic light.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The items present in the image are: cars, persons, bicycles, and a traffic light.\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/home/sohum/CompRobo-VisionLLM/Parsons_PR.jpg\"\n",
    "user_question = \"Please tell me what are the items present in the image.\"\n",
    "response = agent.run(f'{user_question}, this is the image path: {image_path}')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Object detector\",\n",
      "    \"action_input\": \"/home/sohum/CompRobo-VisionLLM/Parsons_PR.jpg\"\n",
      "}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m[518, 40, 582, 110] car 0.9300956726074219\n",
      "[188, 381, 311, 469] car 0.9253584146499634\n",
      "[1068, 223, 1104, 342] person 0.9871610403060913\n",
      "[828, 233, 949, 329] car 0.9450380802154541\n",
      "[1076, 263, 1106, 347] bicycle 0.9070360660552979\n",
      "[635, 71, 713, 135] car 0.9211900234222412\n",
      "[0, 433, 100, 603] car 0.9781932830810547\n",
      "[151, 747, 339, 799] car 0.9839043617248535\n",
      "[389, 267, 493, 367] car 0.9801322817802429\n",
      "[192, 478, 341, 633] car 0.995318591594696\n",
      "[578, 117, 828, 550] traffic light 0.9860709309577942\n",
      "[802, 666, 1028, 798] car 0.982887327671051\n",
      "[0, 639, 84, 799] car 0.9630054831504822\n",
      "[1057, 608, 1199, 766] car 0.965281069278717\n",
      "[988, 218, 1031, 347] person 0.9471637606620789\n",
      "[751, 524, 909, 675] car 0.9911808371543884\n",
      "[489, 560, 670, 749] car 0.9970001578330994\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The bounding boxes of all detected objects in the image are as follows:\\n\\n- Car: [518, 40, 582, 110], [188, 381, 311, 469], [828, 233, 949, 329], [635, 71, 713, 135], [0, 433, 100, 603], [151, 747, 339, 799], [389, 267, 493, 367], [192, 478, 341, 633], [802, 666, 1028, 798], [0, 639, 84, 799], [1057, 608, 1199, 766], [751, 524, 909, 675]\\n- Person: [1068, 223, 1104, 342], [988, 218, 1031, 347]\\n- Bicycle: [1076, 263, 1106, 347]\\n- Traffic light: [578, 117, 828, 550]\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The bounding boxes of all detected objects in the image are as follows:\n",
      "\n",
      "- Car: [518, 40, 582, 110], [188, 381, 311, 469], [828, 233, 949, 329], [635, 71, 713, 135], [0, 433, 100, 603], [151, 747, 339, 799], [389, 267, 493, 367], [192, 478, 341, 633], [802, 666, 1028, 798], [0, 639, 84, 799], [1057, 608, 1199, 766], [751, 524, 909, 675]\n",
      "- Person: [1068, 223, 1104, 342], [988, 218, 1031, 347]\n",
      "- Bicycle: [1076, 263, 1106, 347]\n",
      "- Traffic light: [578, 117, 828, 550]\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/home/sohum/CompRobo-VisionLLM/Parsons_PR.jpg\"\n",
    "user_question = \"Please tell me the bounding boxes of all detected objects in the image.\"\n",
    "response = agent.run(f'{user_question}, this is the image path: {image_path}')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
