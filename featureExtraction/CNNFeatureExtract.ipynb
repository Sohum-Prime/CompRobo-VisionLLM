{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089221a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/zaynpatel/opt/miniconda3/lib/python3.9/site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in /Users/zaynpatel/opt/miniconda3/lib/python3.9/site-packages (0.14.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/zaynpatel/opt/miniconda3/lib/python3.9/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: numpy in /Users/zaynpatel/opt/miniconda3/lib/python3.9/site-packages (from torchvision) (1.22.4)\n",
      "Requirement already satisfied: requests in /Users/zaynpatel/opt/miniconda3/lib/python3.9/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/zaynpatel/opt/miniconda3/lib/python3.9/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zaynpatel/opt/miniconda3/lib/python3.9/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zaynpatel/opt/miniconda3/lib/python3.9/site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zaynpatel/opt/miniconda3/lib/python3.9/site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zaynpatel/opt/miniconda3/lib/python3.9/site-packages (from requests->torchvision) (2023.7.22)\n",
      "\u001b[33mDEPRECATION: amazon-textract-overlayer 0.0.11 has a non-standard dependency specifier Pillow>=9.2.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of amazon-textract-overlayer or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: amazon-textract-overlayer 0.0.11 has a non-standard dependency specifier pypdf>=2.5.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of amazon-textract-overlayer or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ae844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbcdca14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zaynpatel/opt/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/zaynpatel/opt/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e229ad3f",
   "metadata": {},
   "source": [
    "### Prepare images and \"Train\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee9c04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a custom class to add image identifiers to vector representations for better analysis\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class UnderstandImages(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir # this will be the dictionary passed in the custom class UnderstandImages {'className' : 'file/directory/for/class'}\n",
    "        self.transform = transform # transform is the same as the block below - we want to begin converting it into tensor-like format\n",
    "        self.img_names = [] # initializing this as an empty list because I want to iterate over each class directory and list their files\n",
    "\n",
    "        for classname, directoryname in self.img_dir.items():\n",
    "            image_names = os.listdir(directoryname) # list the items in the directory - does this list the items on every loop?\n",
    "            for img_name in image_names:\n",
    "                img_path = os.path.join(directoryname, img_name) # join the directory with the image names\n",
    "                self.img_names.append((img_path, classname))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names) \n",
    "\n",
    "    # override this class so I can get image and filename\n",
    "    def __getitem__(self, idx):\n",
    "        #img_path = os.path.join(self.img_dir, self.img_names[idx]) # is this still right?\n",
    "        img_path, classname = self.img_names[idx] # we need to tuple unpack because we get two items in self.img_names based on our append\n",
    "        image = Image.open(img_path) # convert raw images into tensors\n",
    "        if self.transform: # apply the transform to each image \n",
    "            image = self.transform(image)\n",
    "    \n",
    "        return image, classname # return the image and classname because we want to use these for the labeling later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46f2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224), # crop to the center since we assume most of the important features are in the center\n",
    "    transforms.ToTensor(), # converts a PIL image, like the one defined in the above code block to a PyTorch tensor or np.ndarray\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # normalize based on the values of the resnet when it was originally trained\n",
    "])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd688f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "resnet18.fc = torch.nn.Identity() # remove the final classification layer of the NN so I can just get the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "165d541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np # need a way to persist data so use np array to save items to .npy files too\n",
    "dataset = UnderstandImages(img_dir = '/Users/zaynpatel/vision/visionLLM/CompRobo-VisionLLM/featureExtraction/train', transform=transform) # use the custom defined class to get filename in the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=9, shuffle=False, num_workers=0) # pass in dataset of images, batch_size which is the number of images getting passed to the model, num_workers is number working in parallel on machine\n",
    "\n",
    "resnet18.eval()\n",
    "\n",
    "featuresList = []\n",
    "filenamesList = []\n",
    "# what are they being saved too? Ie - how do I know what next step to take to save items to file?\n",
    "with torch.no_grad():\n",
    "    for inputs, filenames in dataloader: # based on the class we defined, we know there are two inputs to dataloader so we unpack those here for use\n",
    "        features = resnet18(inputs) # apply the resnet on the images\n",
    "        for feature, filename in zip(features, filenames): # zip all output feature vectors and filenames together so we can visualize and compare after\n",
    "            features_np = feature.detach().cpu().numpy().flatten() # error: did features.detch() instead of feature.detach() which meant ...\n",
    "            featuresList.append(features_np)\n",
    "            filenamesList.append(filename)\n",
    "\n",
    "# clarify why convert from list to np.array\n",
    "featuresArray = np.array(featuresList)\n",
    "# clarify why we save as .npz and how np.load() does the unzipping for us\n",
    "np.save('data.npy', featuresArray)\n",
    "\n",
    "with open('filenames.txt', 'w') as f:\n",
    "    for filename in filenamesList:\n",
    "        f.write(f\"{filename}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d78102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these are distances [[ 0.          8.96715998]\n",
      " [ 0.         10.63652766]\n",
      " [ 0.         11.08275723]\n",
      " [ 0.         10.48828647]\n",
      " [ 0.         10.48828647]\n",
      " [ 0.          9.67516517]\n",
      " [ 0.          9.38490528]\n",
      " [ 0.          8.96715998]\n",
      " [ 0.         14.08209505]\n",
      " [ 0.          9.38490528]\n",
      " [ 0.         10.61641669]], and indices [[ 0  7]\n",
      " [ 1  3]\n",
      " [ 2 10]\n",
      " [ 3  4]\n",
      " [ 4  3]\n",
      " [ 5  6]\n",
      " [ 6  9]\n",
      " [ 7  0]\n",
      " [ 8  9]\n",
      " [ 9  6]\n",
      " [10  9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "data = np.load('data.npy', allow_pickle=True) # we said pickle is used because ...\n",
    "with open('filenames.txt', 'r') as f:\n",
    "    filename = f.readlines()\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(data) # ball_tree eliminates datapoints that are too far from the query point, making search faster\n",
    "distances, indices = nbrs.kneighbors(data)\n",
    "print(f'these are distances {distances}, and indices {indices}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "66c511c1ad3b987e9adeb56a6d9aa6aa351c712ec4b3431ede2074b94c691dfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
